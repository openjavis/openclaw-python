"""
æµ‹è¯• Gemini 3 Flash Preview - æ–° API
åŸºäº Google æ¨èçš„ä»£ç ç¤ºä¾‹
"""

import asyncio
import os
from pathlib import Path

from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)


async def test_gemini_3_flash():
    """æµ‹è¯• Gemini 3 Flash with Thinking Mode"""
    
    print("=" * 60)
    print("ğŸš€ Gemini 3 Flash Preview æµ‹è¯•")
    print("=" * 60)
    print()
    
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEY æˆ– GEMINI_API_KEY")
        return
    
    print(f"âœ… API Key å·²åŠ è½½ (é•¿åº¦: {len(api_key)})")
    print()
    
    try:
        from openclaw.agents.providers.gemini_provider import GeminiProvider
        from openclaw.agents.providers.base import LLMMessage
        
        # ä½¿ç”¨ Gemini 3 Flash Preview (æ¨è)
        model_name = "gemini-3-flash-preview"
        
        print(f"ğŸ”§ åˆ›å»º Provider: {model_name}")
        provider = GeminiProvider(
            model=model_name,
            api_key=api_key
        )
        print("âœ… Provider åˆ›å»ºæˆåŠŸ")
        print()
        
        # æµ‹è¯• 1: åŸºç¡€å¯¹è¯
        print("ğŸ“ æµ‹è¯• 1: åŸºç¡€å¯¹è¯")
        print("-" * 60)
        
        messages = [
            LLMMessage(
                role="user",
                content="ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"
            )
        ]
        
        response_parts = []
        async for response in provider.stream(messages, max_tokens=200):
            if response.type == "text_delta":
                text = response.content
                response_parts.append(text)
                print(text, end="", flush=True)
            elif response.type == "done":
                break
        
        print()
        print("âœ… æµ‹è¯• 1 å®Œæˆ")
        print()
        
        # æµ‹è¯• 2: Thinking Mode (Gemini 3 ç‰¹æ€§)
        print("ğŸ“ æµ‹è¯• 2: Thinking Mode (HIGH)")
        print("-" * 60)
        
        messages2 = [
            LLMMessage(
                role="user",
                content="è®¡ç®— 15 çš„å¹³æ–¹æ ¹ï¼Œç„¶åæŠŠç»“æœä¹˜ä»¥ piã€‚è¯·æ˜¾ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚"
            )
        ]
        
        response_parts2 = []
        async for response in provider.stream(
            messages2, 
            max_tokens=500,
            thinking_mode="HIGH"  # å¯ç”¨é«˜çº§æ€è€ƒæ¨¡å¼
        ):
            if response.type == "text_delta":
                text = response.content
                response_parts2.append(text)
                print(text, end="", flush=True)
            elif response.type == "done":
                break
        
        print()
        print("âœ… æµ‹è¯• 2 å®Œæˆ")
        print()
        
        # æµ‹è¯• 3: Google Search (å¯é€‰)
        print("ğŸ“ æµ‹è¯• 3: Google Search é›†æˆ")
        print("-" * 60)
        
        messages3 = [
            LLMMessage(
                role="user",
                content="OpenClaw æ˜¯ä»€ä¹ˆé¡¹ç›®ï¼Ÿè¯·æœç´¢æœ€æ–°ä¿¡æ¯ã€‚"
            )
        ]
        
        response_parts3 = []
        async for response in provider.stream(
            messages3,
            max_tokens=300,
            enable_search=True  # å¯ç”¨ Google æœç´¢
        ):
            if response.type == "text_delta":
                text = response.content
                response_parts3.append(text)
                print(text, end="", flush=True)
            elif response.type == "done":
                break
        
        print()
        print("âœ… æµ‹è¯• 3 å®Œæˆ")
        print()
        
        # æ€»ç»“
        print("=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print()
        print("âœ… æµ‹è¯•ç»“æœ:")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   åŸºç¡€å¯¹è¯: {len(response_parts)} å—")
        print(f"   Thinking Mode: {len(response_parts2)} å—")
        print(f"   Google Search: {len(response_parts3)} å—")
        print()
        print("ğŸš€ Gemini 3 Flash Preview å·¥ä½œæ­£å¸¸ï¼")
        print()
        print("ğŸ’¡ æ¨èé…ç½®:")
        print("   model: gemini-3-flash-preview")
        print("   thinking_mode: HIGH (for complex tasks)")
        print("   enable_search: True (for real-time info)")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


async def list_available_models():
    """åˆ—å‡ºå¯ç”¨çš„ Gemini æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ å¯ç”¨çš„ Gemini æ¨¡å‹")
    print("=" * 60 + "\n")
    
    from openclaw.agents.providers.gemini_provider import GEMINI_MODELS
    
    recommended = []
    stable = []
    others = []
    
    for model_id, info in GEMINI_MODELS.items():
        if "alias" in info:
            continue  # Skip alias entries
        
        if info.get("recommended"):
            recommended.append((model_id, info))
        elif info.get("stable"):
            stable.append((model_id, info))
        else:
            others.append((model_id, info))
    
    print("ğŸŒŸ æ¨èæ¨¡å‹ (Recommended):")
    for model_id, info in recommended:
        features = ", ".join(info.get("features", []))
        print(f"  â€¢ {model_id}")
        print(f"    {info['name']}")
        print(f"    Context: {info['context_window']:,} tokens")
        print(f"    Features: {features}")
        print()
    
    print("âœ¨ ç¨³å®šç‰ˆæœ¬ (Stable):")
    for model_id, info in stable:
        features = ", ".join(info.get("features", []))
        print(f"  â€¢ {model_id}")
        print(f"    {info['name']}")
        print(f"    Context: {info['context_window']:,} tokens")
        print()
    
    print("ğŸ“¦ å…¶ä»–æ¨¡å‹:")
    for model_id, info in others:
        print(f"  â€¢ {model_id} - {info['name']}")


if __name__ == "__main__":
    print()
    print("âš ï¸  å®‰å…¨æé†’:")
    print("   - .env æ–‡ä»¶å·²åœ¨ .gitignore ä¸­")
    print("   - ä¸ä¼šä¸Šä¼ ä»»ä½•æ•æ„Ÿä¿¡æ¯")
    print()
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    asyncio.run(list_available_models())
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_gemini_3_flash())
